# -*- coding: utf-8 -*-
"""Product Review Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DQsGZ96qkGMc3GJag9-5BCGPlhn1GpfT
"""

from google.colab import files #fetch the file
uploaded = files.upload()

import pandas as pd #load the file

df = pd.read_csv('flipkart_reviews_dataset.csv')

!pip install nltk

# Drop rows where 'Reviews' is missing
df_clean = df.dropna(subset=['Reviews']).copy()

# Combine 'Summary' + 'Reviews' into one text column
df_clean['Full_Review'] = df_clean['Summary'].fillna('') + ' ' + df_clean['Reviews']

# Create sentiment label from 'Ratings'
def label_sentiment(rating):
    if rating <= 2.0:
        return 'Negative'
    elif rating == 3.0:
        return 'Neutral'
    else:
        return 'Positive'

df_clean['Sentiment'] = df_clean['Ratings'].apply(label_sentiment)

#checking the merged col
print(df_clean.value_counts())

import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
import re

nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

def preprocess_text(text):
    # Lowercase
    text = text.lower()

    # Remove special characters and numbers
    text = re.sub(r'[^a-zA-Z\s]', '', text)

    # Tokenisation
    tokens = word_tokenize(text)

    # Remove stopwords and lemmatise
    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]

    return ' '.join(tokens)

import nltk
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

nltk.download('omw-1.4')  # For full lemmatization support

import nltk

# Download the 'punkt_tab' data package
nltk.download('punkt_tab')

df_clean['Processed_Review'] = df_clean['Full_Review'].apply(preprocess_text)

from sklearn.feature_extraction.text import TfidfVectorizer

# Create TF-IDF Vectorizer
tfidf = TfidfVectorizer(max_features=5000)  # You can change number of features as needed

# Fit and transform the processed reviews
X = tfidf.fit_transform(df_clean['Processed_Review'])

from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
y = le.fit_transform(df_clean['Sentiment'])  # Converts 'Positive', 'Negative', etc. to numeric labels

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression

# Split into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

lr_model = LogisticRegression(max_iter=1000)  # Increased max_iter for better convergence
lr_model.fit(X_train, y_train)

from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import seaborn as sns
import matplotlib.pyplot as plt

# Predict on test set
y_pred = lr_model.predict(X_test)

# Accuracy
print("Accuracy:", accuracy_score(y_test, y_pred))

# Precision, Recall, F1-score
print("\nClassification Report:\n", classification_report(y_test, y_pred, target_names=le.classes_))

# Confusion matrix
cm = confusion_matrix(y_test, y_pred)

# Plot confusion matrix
plt.figure(figsize=(6, 4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=le.classes_, yticklabels=le.classes_)
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix")
plt.show()

from wordcloud import WordCloud

# Filter reviews by sentiment
positive_reviews = ' '.join(df_clean[df_clean['Sentiment'] == 'Positive']['Processed_Review'])
negative_reviews = ' '.join(df_clean[df_clean['Sentiment'] == 'Negative']['Processed_Review'])
neutral_reviews = ' '.join(df_clean[df_clean['Sentiment'] == 'Neutral']['Processed_Review'])

# Generate WordClouds
positive_wc = WordCloud(width=800, height=400, background_color='white').generate(positive_reviews)
negative_wc = WordCloud(width=800, height=400, background_color='white').generate(negative_reviews)
neutral_wc = WordCloud(width=800, height=400, background_color='white').generate(neutral_reviews)

# Plot the WordClouds
plt.figure(figsize=(15, 5))

plt.subplot(1, 3, 1)
plt.imshow(positive_wc, interpolation='bilinear')
plt.title('Positive Sentiment')
plt.axis('off')

plt.subplot(1, 3, 2)
plt.imshow(negative_wc, interpolation='bilinear')
plt.title('Negative Sentiment')
plt.axis('off')

plt.subplot(1, 3, 3)
plt.imshow(neutral_wc, interpolation='bilinear')
plt.title('Neutral Sentiment')
plt.axis('off')

plt.tight_layout()
plt.show()

print("Confusion Matrix:\n", cm)

sample_reviews = X_test[:10]
sample_preds = lr_model.predict(sample_reviews)

# Show original review, true sentiment, and predicted sentiment
for i in range(sample_reviews.shape[0]): # Use .shape[0] to get the number of rows
    print(f"Review: {df_clean.iloc[i]['Full_Review']}")
    print(f"True Sentiment: {le.inverse_transform([y_test[i]])[0]}")
    print(f"Predicted Sentiment: {le.inverse_transform([sample_preds[i]])[0]}")
    print("-" * 50)